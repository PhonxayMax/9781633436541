{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Initialization chapter 7 - QA across documents",
   "id": "db3e7a670891aef0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T08:38:37.416164Z",
     "start_time": "2025-12-21T08:38:37.406561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Chapter 7 â€” Indexing pipeline (Q&A across documents)\n",
    "- Load many file types from a folder (docx/pdf/txt)\n",
    "- Split -> Embed -> Store in Chroma\n",
    "- OpenRouter-compatible env (fail-fast)\n",
    "\"\"\""
   ],
   "id": "40d3c5c1df0a462a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nChapter 7 â€” Indexing pipeline (Q&A across documents)\\n- Load many file types from a folder (docx/pdf/txt)\\n- Split -> Embed -> Store in Chroma\\n- OpenRouter-compatible env (fail-fast)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-21T10:11:00.389120Z",
     "start_time": "2025-12-21T10:11:00.309263Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import Docx2txtLoader, PyPDFLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T08:38:42.000060Z",
     "start_time": "2025-12-21T08:38:41.993598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# ENV SETUP (fail-fast)\n",
    "# =============================================================================\n",
    "load_dotenv()\n",
    "\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "OPENROUTER_BASE_URL = os.getenv(\"OPENROUTER_BASE_URL\")  # e.g. https://openrouter.ai/api/v1\n",
    "if not OPENROUTER_API_KEY:\n",
    "    raise RuntimeError(\"Missing OPENROUTER_API_KEY in .env\")\n",
    "if not OPENROUTER_BASE_URL:\n",
    "    raise RuntimeError(\"Missing OPENROUTER_BASE_URL in .env\")"
   ],
   "id": "82a86350061a4212",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T08:50:43.627659Z",
     "start_time": "2025-12-21T08:50:42.907309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# =============================================================================\n",
    "# EMBEDDINGS\n",
    "# =============================================================================\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    base_url=OPENROUTER_BASE_URL,\n",
    "    model=\"text-embedding-3-small\",\n",
    ")\n",
    "\n",
    "# Initialize LLM\n",
    "chatbot = ChatOpenAI(\n",
    "    api_key=OPENROUTER_API_KEY,        # API key for authentication\n",
    "    base_url=OPENROUTER_BASE_URL,      # OpenRouter endpoint URL\n",
    "    model=\"openai/gpt-4o\",             # GPT-4o model for high-quality responses\n",
    "    temperature=0,                   # Low temperature for more deterministic, focused answers\n",
    ")"
   ],
   "id": "b6b800292d34edbe",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Indexing pipeline for RAG (chapter 7 - QA across documents)",
   "id": "cff17800c2baec1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T08:38:46.167140Z",
     "start_time": "2025-12-21T08:38:46.023062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# VECTOR STORE (Chroma)\n",
    "# =============================================================================\n",
    "vector_db = Chroma(\n",
    "    collection_name=\"tourist_info\",\n",
    "    embedding_function=embeddings_model,\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")\n"
   ],
   "id": "41b3c91da6693dd7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T08:38:58.241410Z",
     "start_time": "2025-12-21T08:38:58.236826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# SPLITTER\n",
    "# =============================================================================\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=0,\n",
    ")"
   ],
   "id": "7f7172bb15f2862",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T08:39:00.113247Z",
     "start_time": "2025-12-21T08:39:00.109054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# LOADER FACTORY (extension -> loader)\n",
    "# =============================================================================\n",
    "loader_classes = {\n",
    "    \"docx\": Docx2txtLoader,\n",
    "    \"pdf\": PyPDFLoader,\n",
    "    \"txt\": TextLoader,\n",
    "}\n",
    "\n",
    "def get_loader(filename: str):\n",
    "    _, ext = os.path.splitext(filename)\n",
    "    ext = ext.lstrip(\".\").lower()\n",
    "\n",
    "    loader_class = loader_classes.get(ext)\n",
    "    if loader_class:\n",
    "        return loader_class(filename)\n",
    "\n",
    "    raise ValueError(f\"No loader available for file extension '{ext}'\")"
   ],
   "id": "5d503aa43c77bc8e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T08:39:02.007354Z",
     "start_time": "2025-12-21T08:39:02.003377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# INGESTION ORCHESTRATOR\n",
    "# =============================================================================\n",
    "def split_and_import(loader):\n",
    "    docs = loader.load()  # -> list[Document]\n",
    "    chunks = text_splitter.split_documents(docs)  # Document -> list[Document]\n",
    "    vector_db.add_documents(chunks)  # embed + store\n",
    "    print(f\"{loader.__class__.__name__}: {len(chunks)} chunks ingested\")"
   ],
   "id": "ce2fedb3ff801c7d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T08:39:24.261984Z",
     "start_time": "2025-12-21T08:39:06.614431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# INGEST A FOLDER\n",
    "# =============================================================================\n",
    "folder_path = \"CilentoTouristInfo\"\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    if os.path.isfile(file_path):\n",
    "        try:\n",
    "            loader = get_loader(file_path)\n",
    "            print(f\"Loader for {filename}: {loader.__class__.__name__}\")\n",
    "            split_and_import(loader)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "\n",
    "print(\"âœ… Indexing complete.\")"
   ],
   "id": "b27edc1dff15f2a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader for Acciaroli.pdf: PyPDFLoader\n",
      "PyPDFLoader: 6 chunks ingested\n",
      "Loader for Cape Palinuro.txt: TextLoader\n",
      "TextLoader: 4 chunks ingested\n",
      "Loader for Casalvelino.txt: TextLoader\n",
      "TextLoader: 4 chunks ingested\n",
      "Loader for Cilentan coast.docx: Docx2txtLoader\n",
      "Docx2txtLoader: 5 chunks ingested\n",
      "Loader for Cilento Coast Map and Travel Guide.docx: Docx2txtLoader\n",
      "Docx2txtLoader: 24 chunks ingested\n",
      "Loader for Cilento DOC.pdf: PyPDFLoader\n",
      "PyPDFLoader: 3 chunks ingested\n",
      "Loader for Cilento Park.txt: TextLoader\n",
      "TextLoader: 32 chunks ingested\n",
      "Loader for Cilento.docx: Docx2txtLoader\n",
      "Docx2txtLoader: 17 chunks ingested\n",
      "Loader for Cilento.pdf: PyPDFLoader\n",
      "PyPDFLoader: 9 chunks ingested\n",
      "Loader for Marina di Camerota info.pdf: PyPDFLoader\n",
      "PyPDFLoader: 16 chunks ingested\n",
      "Loader for Marina di Camerota.docx: Docx2txtLoader\n",
      "Docx2txtLoader: 9 chunks ingested\n",
      "Loader for NationalParkOfCilentoAndValloDiDiano.txt: TextLoader\n",
      "TextLoader: 9 chunks ingested\n",
      "Loader for Parco Nazionale del Cilento.pdf: PyPDFLoader\n",
      "PyPDFLoader: 10 chunks ingested\n",
      "Loader for Parmenides.docx: Docx2txtLoader\n",
      "Docx2txtLoader: 46 chunks ingested\n",
      "Loader for Santa Maria di Castellabate.docx: Docx2txtLoader\n",
      "Docx2txtLoader: 5 chunks ingested\n",
      "Loader for Velia.pdf: PyPDFLoader\n",
      "PyPDFLoader: 13 chunks ingested\n",
      "âœ… Indexing complete.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ðŸ” à¹€à¸Šà¹‡à¸à¹€à¸žà¸´à¹ˆà¸¡à¹à¸šà¸š â€œà¸žà¸´à¸ªà¸¹à¸ˆà¸™à¹Œ indexâ€ (à¹à¸™à¸°à¸™à¸³à¸¡à¸²à¸)",
   "id": "22d5ad7b0b238286"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T09:17:27.987498Z",
     "start_time": "2025-12-21T09:17:26.963929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "docs = vector_db.similarity_search(\"Cilento Coast\", k=3)\n",
    "for i, d in enumerate(docs, 1):\n",
    "    print(f\"\\n--- hit {i} ---\")\n",
    "    print(\"source:\", d.metadata.get(\"source\"))\n",
    "    print(d.page_content[:300])\n"
   ],
   "id": "997bbca8713c5e4c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- hit 1 ---\n",
      "source: CilentoTouristInfo\\Cilentan coast.docx\n",
      "TheÂ Cilento CoastÂ (Italian:Â Costiera Cilentana) is an Italian stretch of coastline inÂ Cilento, on the southern side of theÂ Province of Salerno. It is situated between the gulfs ofÂ SalernoÂ andÂ Policastro, extending from the municipalities ofÂ Capaccio-PaestumÂ in the north-west, toÂ SapriÂ in the south-e\n",
      "\n",
      "--- hit 2 ---\n",
      "source: CilentoTouristInfo\\Cilento Park.txt\n",
      "Cilento we mean today the southernmost part of the province of Salerno: a strip of plains south of the Sele river and the mountainous bastion between Agropoli, Sapri and Vallo di Diano. Mountains that reach almost 2000 meters and 100 km of mainly rocky coast dotted with bays and coves. The toponym h\n",
      "\n",
      "--- hit 3 ---\n",
      "source: CilentoTouristInfo\\Cilento Park.txt\n",
      "Cilento sea\n",
      "Una caratteristica grotta lungo la costa di Marina di Camerota\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T08:43:36.474658Z",
     "start_time": "2025-12-21T08:43:36.465239Z"
    }
   },
   "cell_type": "code",
   "source": "print(vector_db._collection.count())\n",
   "id": "6cdeb067e38eeee5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  Generation pipeline for RAG (chapter 7 - QA across documents)",
   "id": "2059a8ea6226a20f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T09:15:29.973904Z",
     "start_time": "2025-12-21T09:15:27.333083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# GENERATION PIPELINE (Chapter 7 â€” Q&A / Retrieval + Generation)\n",
    "# - Goal: Ask a question using stored documents in vector_db\n",
    "# - Flow (Figure 7.4 / 7.5):\n",
    "#   question -> retriever -> (top-k Documents)\n",
    "#   Documents -> format_docs -> context string\n",
    "#   {question, context} -> PromptTemplate -> prompt instance\n",
    "#   prompt -> Chat model (LLM) -> answer\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# IMPORTS (Prompt + Runnable)\n",
    "# =============================================================================\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# =============================================================================\n",
    "# 1) PROMPT TEMPLATE (Figure 7.5: Prompt parameters -> PromptTemplate)\n",
    "# - {context}: retrieved text chunks (we will format them into a string)\n",
    "# - {question}: user query\n",
    "# - guardrails: \"don't make up\" + \"3 sentences max\" to reduce hallucination\n",
    "# =============================================================================\n",
    "rag_prompt_template = \"\"\"Use the following pieces of context\n",
    "to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know,\n",
    "don't try to make up an answer.\n",
    "Use three sentences maximum and keep the\n",
    "answer as concise as possible.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "# Create a LangChain PromptTemplate object from the raw template string\n",
    "rag_prompt = PromptTemplate.from_template(rag_prompt_template)\n",
    "\n",
    "# =============================================================================\n",
    "# 2) RETRIEVER (Figure 7.4: Q&A orchestrator -> Retriever -> VectorStore)\n",
    "# - vector_db.as_retriever() wraps the vector store into a Retriever interface\n",
    "# - k=4 means: fetch top 4 most similar chunks\n",
    "# =============================================================================\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# =============================================================================\n",
    "# 3) QUESTION FEEDER (RunnablePassthrough)\n",
    "# - This passes the original question forward in the chain unchanged\n",
    "# - So it can fill the {question} slot in the prompt\n",
    "# =============================================================================\n",
    "question_feeder = RunnablePassthrough()\n",
    "\n",
    "# =============================================================================\n",
    "# 4) CONTEXT FORMATTER (Document list -> string)\n",
    "# - Retriever returns: list[Document]\n",
    "# - PromptTemplate expects: {context} as a string\n",
    "# - We also include \"source\" from metadata so the model can cite it\n",
    "# =============================================================================\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"source: {d.metadata.get('source')}\\n{d.page_content}\"\n",
    "        for d in docs\n",
    "    )\n",
    "\n",
    "# =============================================================================\n",
    "# 5) RAG CHAIN (Figure 7.5: parameters -> prompt -> chat model)\n",
    "#\n",
    "# - The left dict builds the prompt parameters:\n",
    "#   - \"context\": question -> retriever -> list[Document] -> format_docs -> string\n",
    "#   - \"question\": question -> passthrough -> string\n",
    "#\n",
    "# - Then:\n",
    "#   parameters -> rag_prompt (creates prompt instance) -> chatbot (generates answer)\n",
    "# =============================================================================\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": question_feeder}\n",
    "    | rag_prompt\n",
    "    | chatbot\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# 6) EXECUTION HELPER\n",
    "# - invoke(question) runs the whole chain:\n",
    "#   retrieval -> prompt building -> generation\n",
    "# =============================================================================\n",
    "def execute_chain(chain, question):\n",
    "    return chain.invoke(question)\n",
    "\n",
    "# =============================================================================\n",
    "# 7) ASK A QUESTION\n",
    "# - The question includes: \"Also tell me the source.\"\n",
    "# - The model can cite sources because we injected \"source: ...\" into {context}\n",
    "# =============================================================================\n",
    "question = \"\"\"Where was Poseidonia and who renamed\n",
    "it to Paestum? Also tell me the source.\"\"\"\n",
    "\n",
    "answer = execute_chain(rag_chain, question)\n",
    "\n",
    "# =============================================================================\n",
    "# 8) OUTPUT\n",
    "# - ChatOpenAI returns an AIMessage\n",
    "# - The text you want is in answer.content\n",
    "# =============================================================================\n",
    "print(answer.content)\n"
   ],
   "id": "86dd8d944bf185b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poseidonia was located on the coasts of Cilento and was later renamed Paestum by the Lucanians. Source: CilentoTouristInfo\\Cilento Park.txt.\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T09:15:57.028234Z",
     "start_time": "2025-12-21T09:15:54.953071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = \"\"\"And then, what they do?\n",
    "Tell me only if you know.\n",
    "Also tell me the source\"\"\"\n",
    "answer = execute_chain(rag_chain, question)"
   ],
   "id": "5a8454264f18f321",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T09:15:57.593698Z",
     "start_time": "2025-12-21T09:15:57.590720Z"
    }
   },
   "cell_type": "code",
   "source": "print(answer.content)",
   "id": "4e279db9b625fa69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T10:12:24.001509Z",
     "start_time": "2025-12-21T10:12:23.993998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Chapter 7.4 â€” RAG Chain with Chatbot Memory (compatible with your codebase)\n",
    "- Assumes you already have:\n",
    "  - vector_db (Chroma)\n",
    "  - chatbot (ChatOpenAI using OPENROUTER_API_KEY + OPENROUTER_BASE_URL)\n",
    "- Adds:\n",
    "  - ChatPromptTemplate (message-based prompt)\n",
    "  - ChatMessageHistory (memory)\n",
    "  - RunnableLambda(get_messages) to feed latest history each turn\n",
    "\"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "# =============================================================================\n",
    "# 1) MEMORY (chat history container)\n",
    "# =============================================================================\n",
    "chat_history_memory = ChatMessageHistory()\n",
    "\n",
    "# =============================================================================\n",
    "# 2) RETRIEVER (use your vector_db)\n",
    "# =============================================================================\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 4})\n",
    "question_feeder = RunnablePassthrough()\n",
    "\n",
    "# =============================================================================\n",
    "# 3) CONTEXT FORMATTER (list[Document] -> string with sources)\n",
    "# =============================================================================\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"source: {d.metadata.get('source')}\\n{d.page_content}\"\n",
    "        for d in docs\n",
    "    )\n",
    "\n",
    "# =============================================================================\n",
    "# 4) CHAT-BASED PROMPT (with history placeholder)\n",
    "# - \"placeholder\" role is used for inserting message history dynamically\n",
    "# - retrieved_context is injected as an assistant message (same idea as textbook)\n",
    "# =============================================================================\n",
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant, world-class expert in Roman and Greek history, especially in towns located in southern Italy. \"\n",
    "            \"Answer ONLY using what is provided in the retrieved context. If you don't know, say you don't know. \"\n",
    "            \"Use three sentences maximum and keep the answer concise. \"\n",
    "            \"When asked for sources, cite them from lines starting with 'source: ...'.\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{chat_history_messages}\"),\n",
    "        (\"assistant\", \"{retrieved_context}\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# 5) FEED LATEST HISTORY EACH INVOKE (IMPORTANT)\n",
    "# - RunnableLambda ensures we read chat_history_memory.messages at runtime (not snapshot)\n",
    "# =============================================================================\n",
    "def get_messages(_):\n",
    "    return chat_history_memory.messages\n",
    "\n",
    "# =============================================================================\n",
    "# 6) RAG CHAIN (question -> retriever -> format -> prompt -> chatbot)\n",
    "# =============================================================================\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"retrieved_context\": retriever | format_docs,\n",
    "        \"question\": question_feeder,\n",
    "        \"chat_history_messages\": RunnableLambda(get_messages),\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | chatbot\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# 7) EXECUTE WITH MEMORY (update history per turn)\n",
    "# - Store user question\n",
    "# - Invoke chain\n",
    "# - Store AI answer (use answer.content)\n",
    "# =============================================================================\n",
    "def execute_chain_with_memory(chain, question: str):\n",
    "    chat_history_memory.add_user_message(question)\n",
    "\n",
    "    answer = chain.invoke(question)  # -> AIMessage (ChatOpenAI)\n",
    "    chat_history_memory.add_ai_message(answer.content)\n",
    "\n",
    "    return answer\n"
   ],
   "id": "20622a6eec187a1e",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T10:16:49.448204Z",
     "start_time": "2025-12-21T10:16:46.742966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# 8) TEST\n",
    "# =============================================================================\n",
    "q1 = \"Where was Poseidonia and who renamed it to Paestum? Also tell me the source.\"\n",
    "a1 = execute_chain_with_memory(rag_chain, q1)\n",
    "print(a1.content)"
   ],
   "id": "2f7fb6c7ee8cf20",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poseidonia was located on the coasts of the Cilento region in southern Italy. It was renamed to Paestum after being occupied by the Lucanians. \n",
      "\n",
      "source: CilentoTouristInfo\\Velia.pdf\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T10:13:40.072382Z",
     "start_time": "2025-12-21T10:13:37.263597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "q2 = \"And then what did they do? Also tell me the source.\"\n",
    "a2 = execute_chain_with_memory(rag_chain, q2)\n",
    "print(a2.content)"
   ],
   "id": "aac35cca0c4910ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know. The retrieved context does not provide further information on what happened after the Lucanians renamed Poseidonia to Paestum.\n"
     ]
    }
   ],
   "execution_count": 48
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
